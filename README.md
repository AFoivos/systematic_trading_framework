# systematic-trading-framework

A **research-first, systematic trading framework** for quantitative finance, designed to support the full lifecycle from **hypothesis-driven research** to **robust backtesting** and **machine learningâ€“based strategy evaluation**.

This repository prioritizes:

* statistical rigor,
* reproducibility,
* time-aware evaluation,
* and risk-aware modeling.

It is explicitly **not** a collection of ad-hoc trading strategies or heuristic-driven bots.

---

## ğŸ¯ Project Philosophy

Financial markets are **non-stationary, noisy, and regime-dependent**.
Any serious quantitative system must therefore:

1. Respect the **temporal structure** of data
2. Avoid information leakage at all costs
3. Separate **research**, **evaluation**, and **execution logic**
4. Treat **risk management** as a first-class component
5. Benchmark all models against **simple, interpretable baselines**

This framework is built around those principles.

---

## ğŸ§  Core Objectives

* Systematic **alpha research** and feature experimentation
* Time-seriesâ€“aware **backtesting & evaluation**
* Comparison of:

  * statistical models
  * machine learning models
  * deep learning architectures
  * reinforcement learning agents
* Explicit **risk modeling and control**
* Modular, extensible architecture suitable for research â†’ production transition
* **Point-in-time data integrity** (survivorship-bias control, corporate actions, timestamp alignment)
* **Feature store & provenance** for reproducibility (data/feature versioning)
* **Signal aggregation layer** (rank/decay/confidence-weighted sizing)
* **Portfolio optimization with constraints** (market/sector neutrality, turnover caps)
* **Robustness & stress testing** (regime splits, sensitivity checks)
* **Monitoring & drift detection** for production-grade iteration

---

## ğŸ§± Repository Structure

```
quant-research-lab/
â”‚
â”œâ”€â”€ config/                 # YAML configs (models, experiments, backtests)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Immutable raw market data
â”‚   â”œâ”€â”€ processed/          # Cleaned & feature-engineered datasets
â”‚   â””â”€â”€ metadata/           # Contracts, calendars, asset info
â”‚
â”œâ”€â”€ notebooks/              # Exploratory research (EDA, diagnostics)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ features/           # Feature engineering (lags, rolling stats, regimes)
â”‚   â”œâ”€â”€ models/             # Statistical, ML, DL, RL models
â”‚   â”œâ”€â”€ backtesting/        # Time-aware backtesting engine
â”‚   â”œâ”€â”€ risk/               # Position sizing, exposure control, costs
â”‚   â”œâ”€â”€ evaluation/         # Metrics & performance analysis
â”‚   â”œâ”€â”€ signals/            # Signal aggregation (rank/decay/confidence)
â”‚   â”œâ”€â”€ portfolio/          # Portfolio construction & optimization
â”‚   â”œâ”€â”€ monitoring/         # Drift, data quality, and live diagnostics
â”‚   â””â”€â”€ utils/              # Shared utilities
â”‚
â”œâ”€â”€ tests/                  # Unit & integration tests
â”‚
â”œâ”€â”€ logs/                   # Experiment & backtest logs
â”‚
â””â”€â”€ README.md
```

---

## ğŸ³ Docker Workflow (No `venv`)

Use Docker/Compose to run everything inside a containerized Python environment.

Build the image:

```bash
docker compose build
```

Run an interactive shell:

```bash
docker compose run --rm app
```

Run tests:

```bash
docker compose run --rm app pytest
```

Run an experiment:

```bash
docker compose run --rm app python -m src.experiments.runner config/experiments/trend_spy.yaml
```

Notes:

* Source code is mounted into `/workspace`, so local edits are visible immediately in the container.
* You can keep API keys in a local `.env` file (already git-ignored) and pass them to Compose with:

```bash
docker compose --env-file .env run --rm app <command>
```

---

## âš™ï¸ Config-Based Experiments

Define experiments in YAML under `config/` (e.g., `config/experiments/trend_spy.yaml`). Inherit defaults via `extends: base/daily.yaml`. Load and run:

```python
from src.utils.config import load_experiment_config
cfg = load_experiment_config("experiments/trend_spy.yaml")
# then: load_ohlcv(**cfg["data"]), build features per cfg["features"], train model, map signals, run_backtest(...)
```

Keep secrets out of Git: store API keys in env vars and reference them with `data.api_key_env`.

---

## ğŸ“ Modeling Approach

The framework supports and compares multiple modeling paradigms:

### Statistical Models

* ARIMA / SARIMAX
* VAR
* GARCH-style volatility models

### Machine Learning

* Linear & regularized models
* Tree-based models (e.g. gradient boosting)
* Feature importance & explainability analysis

### Deep Learning

* LSTM / temporal CNNs
* Sequence-to-signal architectures
* Strict walk-forward training loops

### Reinforcement Learning

* Custom trading environments
* Risk-aware reward functions
* Policy evaluation under transaction costs

All models:

* operate on **lagged, causal features**
* are evaluated **out-of-sample**
* are benchmarked against naive baselines
* are trained with **purged / embargoed time-series CV** when labels overlap

---

## ğŸ§ª Evaluation & Backtesting

Evaluation follows strict time-series principles:

* âŒ No random train/test splits
* âœ… Walk-forward / expanding window validation
* âœ… Explicit transaction costs & slippage
* âœ… Capital-aware performance accounting
* âœ… **Point-in-time alignment** (avoid lookahead & survivorship leakage)
* âœ… **Robustness checks** (regime splits, sensitivity analysis)

### Key Metrics

* Cumulative & annualized returns
* Sharpe / Sortino ratios
* Maximum drawdown
* Profit factor
* Turnover & stability diagnostics

---

## ğŸ›¡ï¸ Risk Management

Risk is modeled explicitly via:

* position sizing rules
* volatility scaling
* exposure limits
* drawdown-aware constraints
* liquidity-aware cost/impact models

In RL settings, **risk-adjusted reward functions** are used instead of raw returns.

---

## ğŸ” Explainability & Diagnostics

Where applicable, the framework includes:

* feature importance analysis
* regime-conditional performance
* failure mode diagnostics
* model vs baseline attribution
* data quality & drift monitoring hooks

The goal is not just performance, but **understanding**.

---

## ğŸš§ Disclaimer

This repository is intended **solely for research and educational purposes**.

It does **not** constitute financial advice and is **not** designed for live trading without extensive validation, monitoring, and compliance considerations.

---

## ğŸ“Œ Future Extensions

* Live data ingestion layer
* Paper trading & execution simulation
* Portfolio-level optimization
* Advanced regime detection
* Model ensemble & meta-learning
* Alternative data/NLP pipeline (news, filings, embeddings)
* Feature store with data/feature lineage
* Production monitoring & alerting (drift, performance decay)

---

## ğŸ‘¤ Author

Quantitative Research & Machine Learning Engineer
Focus areas: systematic trading, time-series modeling, and ML-driven alpha research.
